# Fake Job Post Detector - Apple-Level ML Pipeline with GenAI Comparison

**A comprehensive machine learning project that detects fake job postings using text analysis, with GenAI benchmarking and real-world deployment.**

## ğŸ¯ Project Overview

This project demonstrates:
- âœ… Practical ML engineering skills
- âœ… Text-based feature extraction and classification  
- âœ… Model explainability with SHAP
- âœ… GenAI integration for benchmarking
- âœ… Deployed UI for real-world usage
- âœ… Apple-level technical storytelling

## ğŸ“Š Dataset

- **Source**: Kaggle "Fake Job Postings" (Real dataset from Shivamb)
- **Size**: 17,880 job posts (17,014 real, 866 fake)
- **Target**: Binary classification (0 = real, 1 = fake)
- **Text Fields**: Combined from title, company_profile, description, requirements, benefits, department
- **Class Imbalance**: 95.2% real jobs, 4.8% fake jobs (realistic for fraud detection)

## ğŸš€ Project Progress

### âœ… Phase 9: Streamlit Web Application (COMPLETED)

**Objective**: Deploy a professional web application for real-world usage

**Completed Tasks**:
1. âœ… Built comprehensive Streamlit app (`app.py`) with professional UI/UX
2. âœ… Implemented real-time job posting classification
3. âœ… Added confidence scores and detailed insights
4. âœ… Created sample data for testing (real and fake job examples)
5. âœ… Integrated model loading with caching for performance
6. âœ… Added error handling and input validation
7. âœ… Created deployment guide (`DEPLOYMENT.md`)

**App Features**:
- **Real-time Classification**: Instant fake job detection
- **Professional UI**: Clean, responsive design with custom CSS
- **Confidence Scores**: Model confidence for each prediction
- **Sample Data**: Built-in test cases for demonstration
- **Error Handling**: Robust input validation and error messages
- **Performance Metrics**: Text length, analysis time tracking
- **Future-Ready**: Framework for GPT-4 integration

**Technical Implementation**:
- **Model Loading**: Cached resource loading for performance
- **Text Preprocessing**: Same pipeline as training data
- **UI Components**: Columns layout, metrics, expandable sections
- **Styling**: Custom CSS for professional appearance
- **Deployment**: Ready for Streamlit Cloud deployment

**Output Files**:
- `app.py` - Main Streamlit application (15KB)
- `DEPLOYMENT.md` - Comprehensive deployment guide (5KB)

**Key Accomplishments**:
- Production-ready web application
- Professional user interface design
- Real-time machine learning inference
- Comprehensive deployment documentation
- Ready for public deployment on Streamlit Cloud

**Usage**:
```bash
# Local development
streamlit run app.py

# Deploy to Streamlit Cloud
# Follow DEPLOYMENT.md instructions
```

## ğŸš€ Project Progress

### âœ… Phase 1: Data Loading and Cleaning (COMPLETED)

**Objective**: Load raw dataset and prepare for ML pipeline

**Completed Tasks**:
1. âœ… Created project structure (`data/`, `model/`, `app/`, `notebooks/`)
2. âœ… Loaded real Kaggle dataset (48MB, 17,880 records)
3. âœ… Implemented comprehensive data cleaning pipeline:
   - Removed data leakage columns (job_id, salary_range, telecommuting, etc.)
   - Combined relevant text fields into single `combined_text` column
   - Applied text cleaning (lowercase, HTML removal, punctuation removal)
   - Preserved only essential columns: `fraudulent` (target) and `combined_text` (features)

**Results**:
- **Final Dataset**: 17,880 posts with 95.2% real jobs, 4.8% fake jobs
- **Text Length**: Average 2,585 characters (range: 14-14,502)
- **Data Quality**: No missing labels, clean text format ready for TF-IDF
- **Output Files**: 
  - `data/cleaned_job_postings.csv` - Clean dataset (44MB)
  - `data/cleaned_job_postings.pkl` - Pickle format for fast loading (45MB)

**Key Accomplishments**:
- Robust text preprocessing pipeline handling real-world data
- Proper data leakage prevention
- Realistic class imbalance (typical for fraud detection)
- Professional code structure with comprehensive documentation
- Successfully processed large-scale dataset (17,880 records)

### âœ… Phase 2: TF-IDF Feature Extraction (COMPLETED)

**Objective**: Extract text features and prepare for model training

**Completed Tasks**:
1. âœ… Loaded cleaned dataset from Phase 1 (17,880 posts)
2. âœ… Validated `combined_text` column (no nulls, proper format)
3. âœ… Applied TF-IDF vectorization with optimized parameters:
   - `max_features=10000` (capped for efficiency)
   - `ngram_range=(1,2)` (unigrams and bigrams)
   - `stop_words='english'` (removed common words)
   - `min_df=2, max_df=0.95` (filtered rare/common terms)
4. âœ… Prepared target variable (`fraudulent` column)
5. âœ… Split data into train/test sets (80/20, stratified)
6. âœ… Saved artifacts for future use

**Results**:
- **Feature Matrix**: 17,880 Ã— 10,000 sparse matrix (1.78% sparsity)
- **Training Set**: 14,304 samples (95.2% real, 4.8% fake)
- **Testing Set**: 3,576 samples (95.2% real, 4.8% fake)
- **Class Imbalance**: 0.051 ratio (severe imbalance - realistic for fraud detection)
- **Output Files**:
  - `model/tfidf_vectorizer.pkl` - TF-IDF vectorizer (408KB)
  - `model/train_test_splits.pkl` - Train/test data (37MB)

**Key Accomplishments**:
- Professional TF-IDF implementation with optimal parameters
- Proper train/test stratification maintaining class balance
- Efficient sparse matrix representation (1.78% sparsity)
- Comprehensive feature extraction with unigrams and bigrams
- Artifact serialization for reproducible pipeline

### âœ… Phase 3: Random Forest Model Training (COMPLETED)

**Objective**: Train and evaluate machine learning model for fake job detection

**Completed Tasks**:
1. âœ… Loaded train/test data from Phase 2 (14,304 train, 3,576 test)
2. âœ… Initialized Random Forest classifier with optimized parameters:
   - `n_estimators=200` (200 trees for robust ensemble)
   - `max_depth=6` (controlled complexity)
   - `class_weight='balanced'` (handles class imbalance)
   - `random_state=42` (reproducibility)
3. âœ… Trained model efficiently (0.30 seconds, 48,153 samples/second)
4. âœ… Comprehensive model evaluation with multiple metrics
5. âœ… Created professional visualizations (ROC, PR curves, confusion matrix)
6. âœ… Generated detailed classification report
7. âœ… Identified top 20 most informative features

**Results**:
- **Model Performance**: 
  - Accuracy: 94.27% (exceeds 80% threshold)
  - ROC-AUC: 0.9720 (exceeds 0.80 threshold)
  - Precision: 0.4494, Recall: 0.8208, F1-Score: 0.5808
- **Training Efficiency**: 0.30 seconds for 14,304 samples
- **Class Performance**: 
  - Real jobs: 99% precision, 95% recall
  - Fake jobs: 45% precision, 82% recall
- **Top Features**: 'passionate', 'position', 'data entry', 'fun', 'team'
- **Output Files**:
  - `models/random_forest_model.pkl` - Trained model (834KB)
  - `reports/classification_report.txt` - Detailed report
  - `plots/roc_curve.png` - ROC curve visualization
  - `plots/pr_curve.png` - Precision-recall curve
  - `plots/confusion_matrix.png` - Confusion matrix heatmap

**Key Accomplishments**:
- Excellent model performance exceeding all thresholds
- Efficient training with Random Forest (alternative to XGBoost)
- Professional evaluation with comprehensive metrics
- High-quality visualizations for model interpretation
- Feature importance analysis revealing key indicators
- Production-ready model serialization

### âœ… Phase 4: SHAP Model Explainability (COMPLETED)

**Objective**: Provide interpretable explanations for model predictions using SHAP

**Completed Tasks**:
1. âœ… Loaded trained Random Forest model and TF-IDF vectorizer
2. âœ… Prepared feature names from vectorizer (10,000 features)
3. âœ… Converted sparse matrices to dense format for SHAP analysis
4. âœ… Initialized SHAP TreeExplainer (optimized for Random Forest)
5. âœ… Computed SHAP values for training data (500 samples) and test data (3,576 samples)
6. âœ… Created comprehensive SHAP visualizations:
   - Beeswarm summary plot showing feature importance distribution
   - Horizontal bar chart of top 20 most important features
   - Waterfall plot for individual prediction explanation
7. âœ… Saved SHAP explainer for future use
8. âœ… Analyzed feature importance with detailed interpretation

**Results**:
- **SHAP Analysis**: Successfully computed SHAP values for 500 training samples
- **Memory Management**: Efficient subsetting to handle large dataset (40MB dense matrix)
- **Feature Insights**: 
  - Most important feature: 'team' (SHAP: -0.0027) - pushes toward real jobs
  - Top fraud indicators: 'recovery', 'experts', 'long term', 'established principles'
  - Top legitimate indicators: 'team', 'growing', 'fun', 'home', 'creative'
- **Visualizations**: Professional SHAP plots with clear feature interpretations
- **Output Files**:
  - `explainability/shap_explainer.pkl` - SHAP explainer (1.25MB)
  - `explainability/shap_summary_plot.png` - Beeswarm plot (327KB)
  - `explainability/shap_feature_importance_bar.png` - Bar chart (233KB)
  - `explainability/shap_waterfall_sample.png` - Waterfall plot (303KB)

**Key Accomplishments**:
- Professional SHAP implementation with TreeExplainer
- Efficient memory management for large-scale explainability
- Comprehensive feature importance analysis
- High-quality visualizations for model interpretation
- Detailed feature interpretation revealing linguistic patterns
- Production-ready explainer serialization

### âœ… Phase 5: 5-fold Cross-Validation and Hyperparameter Tuning (COMPLETED)

**Objective**: Perform comprehensive cross-validation and optimize model hyperparameters

**Completed Tasks**:
1. âœ… Loaded training data from Phase 2 (14,304 samples, 95.2% real, 4.8% fake)
2. âœ… Defined comprehensive parameter grid for RandomizedSearchCV:
   - `n_estimators`: [100, 150, 200, 250, 300]
   - `max_depth`: [4, 6, 8, 10, None]
   - `min_samples_split`: [2, 5, 10]
   - `min_samples_leaf`: [1, 2, 4]
   - `bootstrap`: [True, False]
3. âœ… Setup 5-fold stratified cross-validation with proper class balance
4. âœ… Performed RandomizedSearchCV with 30 iterations (150 total fits)
5. âœ… Comprehensive analysis of cross-validation results
6. âœ… Model comparison between original and tuned versions
7. âœ… Saved optimized model and best parameters

**Results**:
- **Cross-Validation Performance**:
  - Best F1 Score: 0.7708 (Â±0.0437)
  - F1 Scores across 5 folds: [0.8346, 0.7360, 0.8125, 0.7398, 0.7311]
  - Mean F1: 0.7708, Std F1: 0.0437
- **Best Hyperparameters**:
  - `n_estimators`: 200
  - `max_depth`: None (unlimited depth)
  - `min_samples_split`: 10
  - `min_samples_leaf`: 4
  - `bootstrap`: False
- **Search Statistics**: 97.64 seconds for 30 combinations (3.25s average per combination)
- **Performance Improvement**:
  - Accuracy: 94.27% â†’ 98.07% (+3.80%)
  - Precision: 44.94% â†’ 89.39% (+44.46%)
  - F1-Score: 58.08% â†’ 77.38% (+19.30%)
  - ROC-AUC: 97.20% â†’ 98.86% (+1.66%)
- **Output Files**:
  - `models/random_forest_cv.pkl` - Optimized model (12.9MB)
  - `reports/rf_cv_best_params.json` - Best parameters

**Key Accomplishments**:
- Professional cross-validation with stratified sampling
- Comprehensive hyperparameter optimization using RandomizedSearchCV
- Significant performance improvements across all metrics
- Robust model validation with 5-fold cross-validation
- Detailed analysis of parameter combinations and their effects
- Production-ready optimized model with documented best parameters

### âœ… Phase 6: Model Serialization & Optimization (COMPLETED)

**Objective**: Final model evaluation and production-ready serialization

**Completed Tasks**:
1. âœ… Loaded optimized Random Forest model and TF-IDF vectorizer
2. âœ… Evaluated final model on untouched test set (3,576 samples)
3. âœ… Generated comprehensive final visualizations:
   - Final ROC curve with 98.86% AUC
   - Final Precision-Recall curve
   - Final confusion matrix heatmap
4. âœ… Saved final metrics in JSON format with detailed metadata
5. âœ… Created comprehensive model pipeline summary for deployment
6. âœ… Verified all production artifacts are accessible and complete

**Results**:
- **Final Model Performance**:
  - Accuracy: 98.07% (3,489/3,576 correct predictions)
  - F1-Score: 77.38% (balanced precision/recall)
  - Precision: 89.39% (high precision for fraud detection)
  - Recall: 68.21% (good recall for minority class)
  - ROC-AUC: 98.86% (excellent discriminative ability)
- **Confusion Matrix**:
  - True Negatives: 3,389 (real jobs correctly identified)
  - False Positives: 14 (real jobs misclassified as fake)
  - False Negatives: 55 (fake jobs misclassified as real)
  - True Positives: 118 (fake jobs correctly identified)
- **Production Artifacts**:
  - All files verified and accessible
  - Complete deployment documentation
  - Professional visualizations for stakeholders
- **Output Files**:
  - `reports/final_metrics.json` - Final performance metrics (353B)
  - `plots/final_roc_curve.png` - Final ROC curve (169KB)
  - `plots/final_pr_curve.png` - Final PR curve (97KB)
  - `plots/final_confusion_matrix.png` - Final confusion matrix (109KB)
  - `model_pipeline_summary.txt` - Deployment guide (2.7KB)

**Key Accomplishments**:
- Production-ready model with excellent performance
- Comprehensive final evaluation on untouched test set
- Professional visualizations for stakeholder communication
- Complete deployment documentation and pipeline summary
- All artifacts verified and ready for production use
- Robust model handling severe class imbalance effectively

---

## ğŸ“‚ Project Structure

```
fake-job-detector/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ fake_job_postings.csv          # Original Kaggle dataset (48MB)
â”‚   â”œâ”€â”€ cleaned_job_postings.csv       # Phase 1 output (44MB)
â”‚   â””â”€â”€ cleaned_job_postings.pkl       # Fast-loading format (45MB)
â”œâ”€â”€ model/                             # Model artifacts
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl           # Phase 2 output (408KB)
â”‚   â””â”€â”€ train_test_splits.pkl          # Train/test data (37MB)
â”œâ”€â”€ models/                            # Trained models
â”‚   â”œâ”€â”€ random_forest_model.pkl        # Phase 3 output (834KB)
â”‚   â””â”€â”€ random_forest_cv.pkl           # Phase 5 optimized model (12.9MB)
â”œâ”€â”€ reports/                           # Model reports
â”‚   â”œâ”€â”€ classification_report.txt      # Phase 3 report
â”‚   â”œâ”€â”€ rf_cv_best_params.json        # Phase 5 best parameters
â”‚   â””â”€â”€ final_metrics.json             # Phase 6 final metrics
â”œâ”€â”€ plots/                             # Visualizations
â”‚   â”œâ”€â”€ roc_curve.png                  # Phase 3 ROC curve
â”‚   â”œâ”€â”€ pr_curve.png                   # Phase 3 PR curve
â”‚   â”œâ”€â”€ confusion_matrix.png           # Phase 3 confusion matrix
â”‚   â”œâ”€â”€ final_roc_curve.png            # Phase 6 final ROC curve
â”‚   â”œâ”€â”€ final_pr_curve.png             # Phase 6 final PR curve
â”‚   â””â”€â”€ final_confusion_matrix.png     # Phase 6 final confusion matrix
â”œâ”€â”€ explainability/                    # SHAP explainability
â”‚   â”œâ”€â”€ shap_explainer.pkl             # Phase 4 output (1.25MB)
â”‚   â”œâ”€â”€ shap_summary_plot.png          # Beeswarm plot
â”‚   â”œâ”€â”€ shap_feature_importance_bar.png # Feature importance
â”‚   â””â”€â”€ shap_waterfall_sample.png      # Sample prediction
â”œâ”€â”€ app/                              # Streamlit web app (Phase 11)
â”œâ”€â”€ notebooks/                        # Analysis notebooks (Phase 8)
â”œâ”€â”€ phase1_data_cleaning.py           # Phase 1 implementation
â”œâ”€â”€ phase2_tfidf_extraction.py        # Phase 2 implementation
â”œâ”€â”€ phase3_xgboost_training.py        # Phase 3 implementation
â”œâ”€â”€ phase4_shap_explainability.py     # Phase 4 implementation
â”œâ”€â”€ phase5_cross_validation.py        # Phase 5 implementation
â”œâ”€â”€ phase6_model_serialization.py     # Phase 6 implementation
â”œâ”€â”€ model_pipeline_summary.txt        # Deployment guide
â”œâ”€â”€ requirements.txt                  # Python dependencies
â””â”€â”€ README.md                         # This file
```

## ğŸ› ï¸ Technical Stack

- **Language**: Python 3.9
- **Data Processing**: pandas, numpy
- **ML Framework**: scikit-learn, Random Forest
- **Feature Extraction**: TF-IDF with unigrams/bigrams
- **Visualization**: matplotlib, seaborn
- **Explainability**: SHAP (Phase 4)
- **Cross-Validation**: StratifiedKFold, RandomizedSearchCV (Phase 5)
- **Model Serialization**: joblib (Phase 6)
- **GenAI**: OpenAI GPT-4 (Phase 8)
- **Web App**: Streamlit (Phase 11)

## ğŸ”„ Next Phases

- **Phase 7**: GPT-4 Benchmarking
- **Phase 8**: Performance Comparison Analysis
- **Phase 9**: Streamlit Web Application
- **Phase 10**: Documentation & Deployment

---

*Ready for interview-level technical demonstration at Apple and other top-tier tech companies.*
